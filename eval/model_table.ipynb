{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8b568f",
   "metadata": {},
   "source": [
    "### 4. Model Complexity & Practical Usability (모델 복잡도 및 실용성)\n",
    "\n",
    "이 표는 각 모델이 **얼마나 계산적으로 무거운지**, 그리고 실제 사용할 때 **연산/메모리 비용**이 어느 정도인지 비교합니다.\n",
    "\n",
    "| Metric | 의미 (Korean 설명) |\n",
    "|-------|----------------|\n",
    "| **Params (#)** | 학습 가능한 파라미터 총 개수. 모델 표현력 규모를 반영하나, 너무 크면 과적합 및 메모리 비용 증가 가능. |\n",
    "| **FLOPs** | 단일 추론(Forward pass) 동안 수행되는 부동소수점 연산 수. 연산 복잡도의 직접적인 척도. |\n",
    "| **Inference Memory (MB)** | 입력 1개를 추론할 때 GPU 메모리가 어느 정도 사용되는지. |\n",
    "| **Latency per Inference (s)** | 입력 하나를 처리하는 데 걸리는 시간. 실시간 처리 가능성 및 배치 사이즈 결정에 영향. |\n",
    "\n",
    "#### 해석 관점\n",
    "- **ViT** 계열은 일반적으로 **파라미터 수는 크지만 FLOPs 효율이 좋아** 추론 속도는 빠른 편.\n",
    "- **UNet3D (V-NET)** 는 **입체 convolution 핵심 구조로 인해 메모리 사용량이 크고 추론 시간이 상대적으로 길 수 있음.**\n",
    "- **Base Model** 은 구조가 단순하므로 일반적으로 가장 가볍지만 성능 한계가 존재.\n",
    "\n",
    "즉,\n",
    "> 이 표는 “**정확도 vs 계산비용**” 트레이드오프를 정량적으로 보여주며,  \n",
    "> 실제 운용 환경에서 어떤 모델을 선택해야 하는지를 결정하는 핵심 기준이 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105bbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:49:38.163184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-06 10:49:45.172994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# <<< 이 셀을 노트북 \"맨 위\"에서 실행하세요 >>>\n",
    "import os\n",
    "# TF가 GPU를 전혀 보지 못하도록 비활성화 (CPU 강제)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "# GPU가 안 보이므로 굳이 메모리 그로스 설정은 불필요\n",
    "\n",
    "# PyTorch는 별도 환경에서 GPU 사용 (CUDA_VISIBLE_DEVICES가 빈 문자열이면 CPU만 보임)\n",
    "# -> Torch쪽에서는 다시 원하는 GPU를 지정해서 사용하세요 (SLURM 스크립트 등에서 지정)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1d0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Choose a big GPU (e.g., 1) BEFORE importing TF/Torch =====\n",
    "import os\n",
    "GPU_ID = \"1\"  # <- 20GB 있는 GPU로 지정 (원하면 \"2\",\"3\"로 바꿔도 됨)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_ID\n",
    "os.environ.setdefault(\"TF_GPU_ALLOCATOR\", \"cuda_malloc_async\")  # TF 메모리 파편화 완화\n",
    "\n",
    "# (이 아래부터 TensorFlow / PyTorch import)\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth on visible GPUs\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    for g in gpus:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    TF_DEVICE = \"/GPU:0\" if gpus else \"/CPU:0\"   # CUDA_VISIBLE_DEVICES로 remap된 0번\n",
    "except Exception:\n",
    "    TF_DEVICE = \"/CPU:0\"\n",
    "\n",
    "import torch\n",
    "TORCH_DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # 동일하게 remap된 0\n",
    "\n",
    "# ----- TF latency/memory (GPU/CPU 자동) -----\n",
    "def tf_infer_latency_and_memory(model, input_shape=(1,2,128,128,128), warmup=2, runs=5):\n",
    "    x = tf.random.normal(input_shape, dtype=tf.float32)\n",
    "    call_fn = tf.function(model, jit_compile=False)\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = call_fn(x, training=False)\n",
    "\n",
    "    # Memory (best-effort)\n",
    "    mem_mb = float(\"nan\")\n",
    "    try:\n",
    "        info0 = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "    except Exception:\n",
    "        info0 = None\n",
    "\n",
    "    t0 = tf.timestamp()\n",
    "    for _ in range(runs):\n",
    "        _ = call_fn(x, training=False)\n",
    "    t1 = tf.timestamp()\n",
    "\n",
    "    if info0 is not None:\n",
    "        try:\n",
    "            info1 = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "            peak = max(info0.get(\"peak\", 0), info1.get(\"peak\", 0))\n",
    "            mem_mb = float(peak) / (1024**2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return float(t1 - t0) / runs, mem_mb\n",
    "\n",
    "# ----- Torch latency/memory/FLOPs (동일한 logical cuda:0 사용) -----\n",
    "def torch_infer_latency(model, input_shape=(1,2,128,128,128), device=TORCH_DEVICE, warmup=3, runs=5):\n",
    "    model = model.to(device).eval()\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "    if device.startswith(\"cuda\"):\n",
    "        torch.cuda.synchronize()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "        if device.startswith(\"cuda\"):\n",
    "            torch.cuda.synchronize()\n",
    "        import time\n",
    "        t0 = time.time()\n",
    "        for _ in range(runs):\n",
    "            _ = model(x)\n",
    "        if device.startswith(\"cuda\"):\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "    return float((t1 - t0) / runs)\n",
    "\n",
    "def torch_infer_memory(model, input_shape=(1,2,128,128,128), device=TORCH_DEVICE):\n",
    "    if not device.startswith(\"cuda\"):\n",
    "        return float(\"nan\")\n",
    "    model = model.to(device).eval()\n",
    "    x = torch.randn(*input_shape, device=device)\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    with torch.inference_mode():\n",
    "        _ = model(x)\n",
    "    return torch.cuda.max_memory_allocated(device) / (1024**2)\n",
    "\n",
    "try:\n",
    "    from thop import profile, clever_format\n",
    "    def torch_try_flops(model, input_shape=(1,2,128,128,128), device=TORCH_DEVICE):\n",
    "        model = model.to(device).eval()\n",
    "        x = torch.randn(*input_shape, device=device)\n",
    "        macs, _ = profile(model, inputs=(x,), verbose=False)\n",
    "        flops_val = macs * 2\n",
    "        flops_str, _ = clever_format([flops_val, macs], \"%.3f\")\n",
    "        return flops_str\n",
    "except Exception:\n",
    "    def torch_try_flops(*args, **kwargs):\n",
    "        return \"N/A\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82851c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:50:03.936396: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2025-11-06 10:50:03.936583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20657 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:48:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Base model: <class 'keras.src.engine.functional.Functional'>\n",
      "✅ UNet3D: <class 'UNet3D_mod.UNet3D'>\n",
      "✅ ViT: <class 'VoxelViTUNet3D_mod.VoxelViTUNet3D'>\n"
     ]
    }
   ],
   "source": [
    "# ===== Safe loader: import class/function from an exact file path =====\n",
    "import importlib.util, types, sys\n",
    "\n",
    "def load_symbol_from_file(py_path: str, symbol_name: str):\n",
    "    spec = importlib.util.spec_from_file_location(f\"{symbol_name}_mod\", py_path)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise ImportError(f\"Cannot load spec for {py_path}\")\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)  # type: ignore\n",
    "    if not hasattr(mod, symbol_name):\n",
    "        raise ImportError(f\"{symbol_name} not found in {py_path}\")\n",
    "    return getattr(mod, symbol_name)\n",
    "\n",
    "# ===== Exact paths & class names =====\n",
    "BASE_PATH = \"/scratch/adupuy/CF4_CNN/generatorSingle.py\"            # TF Generator()\n",
    "UNET_PATH = \"/home/mingyeong/GAL2DM_ASIM_VNET/src/model.py\"         # UNet3D\n",
    "VIT_PATH  = \"/home/mingyeong/GAL2DM_ASIM_ViT/src/model.py\"          # VoxelViTUNet3D\n",
    "\n",
    "SYM_BASE = \"Generator\"\n",
    "SYM_UNET = \"UNet3D\"\n",
    "SYM_VIT  = \"VoxelViTUNet3D\"\n",
    "\n",
    "# ===== Load TensorFlow Base Model =====\n",
    "import tensorflow as tf\n",
    "Generator = load_symbol_from_file(BASE_PATH, SYM_BASE)\n",
    "base_model_tf = Generator()     # returns tf.keras.Model, input=(B,2,128,128,128)\n",
    "\n",
    "# ===== Load PyTorch models =====\n",
    "import torch\n",
    "UNet3D = load_symbol_from_file(UNET_PATH, SYM_UNET)\n",
    "VoxelViTUNet3D = load_symbol_from_file(VIT_PATH, SYM_VIT)\n",
    "\n",
    "# UNet3D uses in_ch, out_ch\n",
    "unet_model_torch = UNet3D(in_ch=2, out_ch=1)\n",
    "\n",
    "# VoxelViTUNet3D: try reasonable constructor signatures\n",
    "def instantiate_vit(cls):\n",
    "    try:\n",
    "        return cls(in_ch=2, out_ch=1)      # most common case\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        return cls(in_channels=2, out_channels=1)\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return cls()    # fallback → internal defaults\n",
    "\n",
    "vit_model_torch = instantiate_vit(VoxelViTUNet3D)\n",
    "\n",
    "print(\"✅ Base model:\", type(base_model_tf))\n",
    "print(\"✅ UNet3D:\", type(unet_model_torch))\n",
    "print(\"✅ ViT:\", type(vit_model_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5795af07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating V-NET (UNet3D) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating ViT (3D Transformer) ---\n",
      "\n",
      "=== Model Complexity & Practical Usability (PyTorch) ===\n",
      "\n",
      "               Model  Params (#)    FLOPs    MACs  Inference Memory (MB)  Latency per Inference (s)\n",
      "      V-NET (UNet3D)    28823577 121.235G 60.617G             854.778809                   0.030408\n",
      "ViT (3D Transformer)    23485633   2.282T  1.141T            3010.214355                   0.120237\n",
      "\n",
      "Saved → model_complexity_summary_torch.csv\n",
      "\n",
      "[TensorFlow] Base Model params: 461,006,711\n"
     ]
    }
   ],
   "source": [
    "# ==== Consistent names from your loaders/constructors ====\n",
    "# TensorFlow\n",
    "base_model_tf = base_model_tf      # already created above\n",
    "\n",
    "# PyTorch\n",
    "unet_model = unet_model_torch\n",
    "vit_model  = vit_model_torch\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "from thop import profile, clever_format  # pip install thop\n",
    "\n",
    "def count_params(model: torch.nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def _ensure_device(device: str) -> torch.device:\n",
    "    if device.startswith(\"cuda\"):\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"[WARN] CUDA requested but not available. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "        # support e.g. \"cuda:0\"\n",
    "        return torch.device(device)\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def measure_inference_memory(model: torch.nn.Module,\n",
    "                             input_shape=(1,2,128,128,128),\n",
    "                             device=\"cuda\") -> float:\n",
    "    dev = _ensure_device(device)\n",
    "    model = model.to(dev).eval()\n",
    "    dummy = torch.randn(*input_shape, device=dev)\n",
    "\n",
    "    if dev.type != \"cuda\":\n",
    "        return float(\"nan\")  # GPU-only metric\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats(dev)\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy)\n",
    "    return torch.cuda.max_memory_allocated(dev) / (1024**2)\n",
    "\n",
    "def measure_latency(model: torch.nn.Module,\n",
    "                    input_shape=(1,2,128,128,128),\n",
    "                    device=\"cuda\",\n",
    "                    warmup=3, runs=5) -> float:\n",
    "    dev = _ensure_device(device)\n",
    "    model = model.to(dev).eval()\n",
    "    dummy = torch.randn(*input_shape, device=dev)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(dummy)\n",
    "\n",
    "        if dev.type == \"cuda\":\n",
    "            torch.cuda.synchronize(dev)\n",
    "        t0 = time.time()\n",
    "        for _ in range(runs):\n",
    "            _ = model(dummy)\n",
    "        if dev.type == \"cuda\":\n",
    "            torch.cuda.synchronize(dev)\n",
    "\n",
    "    return (time.time() - t0) / runs\n",
    "\n",
    "def evaluate_model_complexity(model_dict, input_shape=(1,2,128,128,128), device=\"cuda\"):\n",
    "    dev = _ensure_device(device)\n",
    "    results = []\n",
    "    for name, model in model_dict.items():\n",
    "        print(f\"\\n--- Evaluating {name} ---\")\n",
    "        # Params\n",
    "        try:\n",
    "            params = count_params(model)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Param count failed for {name}: {e}\")\n",
    "            params = float(\"nan\")\n",
    "\n",
    "        # FLOPs / MACs via thop (may fail for some custom layers)\n",
    "        try:\n",
    "            dummy = torch.randn(*input_shape, device=dev)\n",
    "            model_eval = model.to(dev).eval()\n",
    "            macs_raw, _ = profile(model_eval, inputs=(dummy,), verbose=False)\n",
    "            # FLOPs ~= 2 * MACs is a common convention for convs\n",
    "            flops_str, macs_str = clever_format([macs_raw * 2, macs_raw], \"%.3f\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] THOP profiling failed for {name}: {e}\")\n",
    "            flops_str, macs_str = \"NaN\", \"NaN\"\n",
    "\n",
    "        # Inference memory (GPU only)\n",
    "        try:\n",
    "            mem = measure_inference_memory(model, input_shape, device)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Memory measurement failed for {name}: {e}\")\n",
    "            mem = float(\"nan\")\n",
    "\n",
    "        # Latency\n",
    "        try:\n",
    "            latency = measure_latency(model, input_shape, device)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Latency measurement failed for {name}: {e}\")\n",
    "            latency = float(\"nan\")\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Params (#)\": params,\n",
    "            \"FLOPs\": flops_str,\n",
    "            \"MACs\": macs_str,\n",
    "            \"Inference Memory (MB)\": mem,\n",
    "            \"Latency per Inference (s)\": latency,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# --------------------- Run (PyTorch-only) ---------------------\n",
    "model_dict = {\n",
    "    \"V-NET (UNet3D)\": unet_model,\n",
    "    \"ViT (3D Transformer)\": vit_model,\n",
    "}\n",
    "df_complex = evaluate_model_complexity(model_dict, input_shape=(1,2,128,128,128), device=\"cuda\")\n",
    "\n",
    "print(\"\\n=== Model Complexity & Practical Usability (PyTorch) ===\\n\")\n",
    "print(df_complex.to_string(index=False))\n",
    "df_complex.to_csv(\"model_complexity_summary_torch.csv\", index=False)\n",
    "print(\"\\nSaved → model_complexity_summary_torch.csv\")\n",
    "\n",
    "# --------------------- Optional: TensorFlow base model summary ---------------------\n",
    "try:\n",
    "    # Keras parameter count (trainable + non-trainable)\n",
    "    tf_params = int(base_model_tf.count_params())\n",
    "    print(f\"\\n[TensorFlow] Base Model params: {tf_params:,}\")\n",
    "    # If you need TF FLOPs/latency, profile separately with tf.profiler or tf.function jit; not compatible with thop.\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] TensorFlow base model summary failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74809b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "WARNING:root:mode trilinear is not implemented yet, take it a zero op\n",
      "2025-11-06 10:56:12.142539: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2025-11-06 10:56:12.142819: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2025-11-06 10:56:12.151411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20657 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:48:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mingyeong/miniconda3/envs/torch/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:5232: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mingyeong/miniconda3/envs/torch/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:5232: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          \n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/1839.50b flops)\n",
      "  model/conv3d_8/Conv3D (695.78b/695.78b flops)\n",
      "  model/conv3d_7/Conv3D (347.89b/347.89b flops)\n",
      "  model/conv3d_1/Conv3D (268.44b/268.44b flops)\n",
      "  model/conv3d_6/Conv3D (173.95b/173.95b flops)\n",
      "  model/conv3d_2/Conv3D (134.22b/134.22b flops)\n",
      "  model/conv3d_5/Conv3D (86.97b/86.97b flops)\n",
      "  model/conv3d_3/Conv3D (67.11b/67.11b flops)\n",
      "  model/conv3d_4/Conv3D (33.55b/33.55b flops)\n",
      "  model/conv3d/Conv3D (16.78b/16.78b flops)\n",
      "  model/conv3d_9/Conv3D (14.72b/14.72b flops)\n",
      "  model/conv3d/BiasAdd (33.55m/33.55m flops)\n",
      "  model/conv3d_8/BiasAdd (33.55m/33.55m flops)\n",
      "  model/conv3d_1/BiasAdd (8.39m/8.39m flops)\n",
      "  model/conv3d_7/BiasAdd (8.39m/8.39m flops)\n",
      "  model/conv3d_2/BiasAdd (2.10m/2.10m flops)\n",
      "  model/conv3d_6/BiasAdd (2.10m/2.10m flops)\n",
      "  model/conv3d_9/BiasAdd (2.10m/2.10m flops)\n",
      "  model/conv3d_3/BiasAdd (524.29k/524.29k flops)\n",
      "  model/conv3d_5/BiasAdd (524.29k/524.29k flops)\n",
      "  model/conv3d_4/BiasAdd (131.07k/131.07k flops)\n",
      "\n",
      "======================End of Report==========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:57:16.179260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2025-11-06 10:57:16.280948: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Complexity & Practical Usability (All) ===\n",
      "\n",
      "               Model   Params    FLOPs    MACs        Mem    Latency\n",
      "      V-NET (UNet3D)  28.824M 121.235G 60.617G 937.386 MB  30.377 ms\n",
      "ViT (3D Transformer)  23.502M   2.282T  1.141T     2.994K 120.046 ms\n",
      "     Base Model (TF) 461.007M   1.840T     NaN     4.244K 247.109 ms\n",
      "\n",
      "Saved → model_complexity_summary_all.csv\n"
     ]
    }
   ],
   "source": [
    "import os, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from thop import profile, clever_format\n",
    "\n",
    "# ========= Unit formatters =========\n",
    "def fmt_si(x, kind=\"count\"):\n",
    "    if x is None or (isinstance(x, float) and (math.isnan(x) or math.isinf(x))):\n",
    "        return \"NaN\"\n",
    "    scales = [\n",
    "        (1e12, \"T\"),\n",
    "        (1e9,  \"G\"),\n",
    "        (1e6,  \"M\"),\n",
    "        (1e3,  \"K\"),\n",
    "    ]\n",
    "    for s, tag in scales:\n",
    "        if abs(x) >= s:\n",
    "            val = x / s\n",
    "            return f\"{val:.3f}{tag}\"\n",
    "    if kind == \"time_ms\":\n",
    "        return f\"{x*1e3:.3f} ms\"\n",
    "    if kind == \"mem\":\n",
    "        return f\"{x:.3f} MB\"\n",
    "    return f\"{x:.0f}\"\n",
    "\n",
    "def fmt_params(n):  # counts -> M/B\n",
    "    if n is None or (isinstance(n, float) and (math.isnan(n) or math.isinf(n))):\n",
    "        return \"NaN\"\n",
    "    if n >= 1e9:\n",
    "        return f\"{n/1e9:.3f}B\"\n",
    "    if n >= 1e6:\n",
    "        return f\"{n/1e6:.3f}M\"\n",
    "    return f\"{n:,}\"\n",
    "\n",
    "# ========= PyTorch profiling =========\n",
    "def _ensure_device(device: str) -> torch.device:\n",
    "    if device.startswith(\"cuda\"):\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"[WARN] CUDA requested but not available. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "        return torch.device(device)\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def torch_profile(model: torch.nn.Module,\n",
    "                  input_shape=(1,2,128,128,128),\n",
    "                  device=\"cuda\",\n",
    "                  warmup=3, runs=5):\n",
    "    dev = _ensure_device(device)\n",
    "    model = model.to(dev).eval()\n",
    "    x = torch.randn(*input_shape, device=dev)\n",
    "\n",
    "    # Params\n",
    "    try:\n",
    "        params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    except Exception:\n",
    "        params = float(\"nan\")\n",
    "\n",
    "    # FLOPs/MACs\n",
    "    try:\n",
    "        macs_raw, _ = profile(model, inputs=(x,), verbose=False)\n",
    "        flops_str, macs_str = clever_format([macs_raw * 2, macs_raw], \"%.3f\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] THOP failed: {e}\")\n",
    "        flops_str, macs_str = \"NaN\", \"NaN\"\n",
    "\n",
    "    # GPU mem\n",
    "    try:\n",
    "        if dev.type == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats(dev)\n",
    "            with torch.no_grad():\n",
    "                _ = model(x)\n",
    "            mem_mb = torch.cuda.max_memory_allocated(dev) / (1024**2)\n",
    "        else:\n",
    "            mem_mb = float(\"nan\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Torch mem failed: {e}\")\n",
    "        mem_mb = float(\"nan\")\n",
    "\n",
    "    # Latency\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for _ in range(warmup):\n",
    "                _ = model(x)\n",
    "            if dev.type == \"cuda\":\n",
    "                torch.cuda.synchronize(dev)\n",
    "            t0 = time.time()\n",
    "            for _ in range(runs):\n",
    "                _ = model(x)\n",
    "            if dev.type == \"cuda\":\n",
    "                torch.cuda.synchronize(dev)\n",
    "        lat_s = (time.time() - t0) / runs\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Torch latency failed: {e}\")\n",
    "        lat_s = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"Params (#)\": params,\n",
    "        \"FLOPs\": flops_str,\n",
    "        \"MACs\": macs_str,\n",
    "        \"Inference Memory (MB)\": mem_mb,\n",
    "        \"Latency per Inference (s)\": lat_s,\n",
    "    }\n",
    "\n",
    "# ========= TensorFlow profiling =========\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n",
    "\n",
    "def _tf_input_shape_from_model(model, fallback=(1,128,128,128,2)):\n",
    "    try:\n",
    "        shp = model.input_shape\n",
    "        if isinstance(shp, (list, tuple)):\n",
    "            if isinstance(shp[0], (list, tuple)):  # multiple inputs\n",
    "                shp = shp[0]\n",
    "        # replace None batch with 1\n",
    "        shp = tuple(1 if (d is None) else d for d in shp)\n",
    "        if len(shp) == 5:\n",
    "            return shp  # assume model is correct (5D)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return fallback\n",
    "\n",
    "def tf_profile(model: tf.keras.Model,\n",
    "               input_shape=None,\n",
    "               device=\"/GPU:0\",\n",
    "               warmup=3, runs=5):\n",
    "    # Input shape\n",
    "    if input_shape is None:\n",
    "        input_shape = _tf_input_shape_from_model(model)\n",
    "\n",
    "    # Determine channels-last vs channels-first from shape\n",
    "    # Expect either (B,D,H,W,C) or (B,C,D,H,W)\n",
    "    if len(input_shape) != 5:\n",
    "        raise ValueError(f\"TF model expected 5D input, got {input_shape}\")\n",
    "    if input_shape[-1] in (1,2,3,4):\n",
    "        layout = \"channels_last\"\n",
    "    else:\n",
    "        layout = \"channels_first\"\n",
    "\n",
    "    # Build dummy\n",
    "    x_np = np.random.randn(*input_shape).astype(np.float32)\n",
    "    x_tf = tf.constant(x_np)\n",
    "\n",
    "    # Params\n",
    "    try:\n",
    "        params = int(model.count_params())\n",
    "    except Exception:\n",
    "        params = float(\"nan\")\n",
    "\n",
    "    # FLOPs via TF profiler (best effort)\n",
    "    def _flops_estimate_keras(model, sample):\n",
    "        try:\n",
    "            @tf.function(jit_compile=False)\n",
    "            def _call(inp):\n",
    "                return model(inp, training=False)\n",
    "\n",
    "            concrete = _call.get_concrete_function(sample)\n",
    "            frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete)\n",
    "            # Use TF v1 profiler on graph_def\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "            flops = tf.compat.v1.profiler.profile(\n",
    "                graph=frozen_func.graph,\n",
    "                options=opts\n",
    "            )\n",
    "            return float(flops.total_float_ops) if flops is not None else float(\"nan\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] TF FLOPs profiling failed: {e}\")\n",
    "            return float(\"nan\")\n",
    "\n",
    "    flops_total = _flops_estimate_keras(model, x_tf)  # raw FLOPs (approx)\n",
    "    flops_str = fmt_si(flops_total, kind=\"count\") if not np.isnan(flops_total) else \"NaN\"\n",
    "    macs_str  = \"NaN\"  # TF v1 profiler reports FLOPs, not MACs\n",
    "\n",
    "    # GPU memory (best effort; only works on TF>=2.9 and with GPU)\n",
    "    mem_mb = float(\"nan\")\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "        if gpus:\n",
    "            # Clear peak then run once\n",
    "            # Note: TF doesn't expose a reset; we sample before/after instead.\n",
    "            _ = model(x_tf, training=False)\n",
    "            info = tf.config.experimental.get_memory_info(\"GPU:0\")\n",
    "            # peak memory since the start of program in bytes\n",
    "            mem_mb = info.get(\"peak\", np.nan) / (1024**2)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] TF memory query failed: {e}\")\n",
    "\n",
    "    # Latency\n",
    "    try:\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x_tf, training=False)\n",
    "        t0 = time.time()\n",
    "        for _ in range(runs):\n",
    "            _ = model(x_tf, training=False)\n",
    "        lat_s = (time.time() - t0) / runs\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] TF latency failed: {e}\")\n",
    "        lat_s = float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"Params (#)\": params,\n",
    "        \"FLOPs\": flops_str,\n",
    "        \"MACs\": macs_str,\n",
    "        \"Inference Memory (MB)\": mem_mb,\n",
    "        \"Latency per Inference (s)\": lat_s,\n",
    "        \"layout\": layout,\n",
    "        \"input_shape_used\": input_shape,\n",
    "    }\n",
    "\n",
    "# ========= Run all & pretty print =========\n",
    "torch_models = {\n",
    "    \"V-NET (UNet3D)\": unet_model,\n",
    "    \"ViT (3D Transformer)\": vit_model,\n",
    "}\n",
    "tf_models = {\n",
    "    \"Base Model (TF)\": base_model_tf,\n",
    "}\n",
    "\n",
    "torch_rows = []\n",
    "for name, m in torch_models.items():\n",
    "    res = torch_profile(m, input_shape=(1,2,128,128,128), device=\"cuda\")\n",
    "    torch_rows.append({\"Model\": name, **res})\n",
    "\n",
    "tf_rows = []\n",
    "for name, m in tf_models.items():\n",
    "    # If your TF model expects (B,2,128,128,128) (channels-first), pass that here:\n",
    "    # tf_input = (1,2,128,128,128)\n",
    "    res = tf_profile(m, input_shape=None, device=\"/GPU:0\")\n",
    "    tf_rows.append({\"Model\": name, **res})\n",
    "\n",
    "df_all = pd.DataFrame(torch_rows + tf_rows,\n",
    "                      columns=[\"Model\",\"Params (#)\",\"FLOPs\",\"MACs\",\"Inference Memory (MB)\",\"Latency per Inference (s)\"])\n",
    "\n",
    "# Human-readable view\n",
    "df_pretty = pd.DataFrame({\n",
    "    \"Model\": df_all[\"Model\"],\n",
    "    \"Params\": [fmt_params(x) if isinstance(x,(int,float)) else x for x in df_all[\"Params (#)\"]],\n",
    "    \"FLOPs\":  df_all[\"FLOPs\"],\n",
    "    \"MACs\":   df_all[\"MACs\"],\n",
    "    \"Mem\":    [fmt_si(x, \"mem\") if isinstance(x,(int,float)) else x for x in df_all[\"Inference Memory (MB)\"]],\n",
    "    \"Latency\": [fmt_si(x, \"time_ms\") if isinstance(x,(int,float)) else x for x in df_all[\"Latency per Inference (s)\"]],\n",
    "})\n",
    "\n",
    "print(\"\\n=== Model Complexity & Practical Usability (All) ===\\n\")\n",
    "print(df_pretty.to_string(index=False))\n",
    "\n",
    "df_all.to_csv(\"model_complexity_summary_all.csv\", index=False)\n",
    "print(\"\\nSaved → model_complexity_summary_all.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
