{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_three_models.py\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# -------------------------------\n",
    "# User configuration\n",
    "# -------------------------------\n",
    "INDEX = 100                                   # Which test sample to visualize\n",
    "RUN_STEM = \"20251014_024804\"                  # Must match your training/prediction run folder name\n",
    "PROJECT_ROOT = \"/home/mingyeong/2510_GAL2DM_ASIM_ViT\"\n",
    "\n",
    "TEST_FILE = f\"/scratch/adupuy/cosmicweb_asim/ASIM_TSC/samples/test/{INDEX}.hdf5\"\n",
    "CKPT_BASE = os.path.join(PROJECT_ROOT, \"results/vit\", RUN_STEM)\n",
    "PRED_BASE = os.path.join(PROJECT_ROOT, \"results/vit_predictions\", RUN_STEM)\n",
    "\n",
    "TAGS = [\"patch8\", \"patch4\", \"patch2\"]         # Subfolders for the three experiments\n",
    "SLICE_AXIS = 2                                # 0: D, 1: H, 2: W\n",
    "SLICE_INDEX_MODE = \"center\"                   # \"center\" or int index\n",
    "\n",
    "OUT_DIR = os.path.join(PROJECT_ROOT, \"figs\", \"viz_models\", RUN_STEM)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "def _safesqueeze(arr):\n",
    "    a = np.squeeze(arr)\n",
    "    if a.ndim < 3:\n",
    "        raise ValueError(f\"Unexpected ndim after squeeze: {a.ndim}, shape={a.shape}\")\n",
    "    return a\n",
    "\n",
    "def _ensure_input_shape(x):\n",
    "    \"\"\"\n",
    "    Accept:\n",
    "      (2,D,H,W) or (1,2,D,H,W) or (N,2,D,H,W) with N=1\n",
    "    Return:\n",
    "      (2,D,H,W)\n",
    "    \"\"\"\n",
    "    arr = x\n",
    "    if arr.ndim == 4 and arr.shape[0] == 2:\n",
    "        return arr\n",
    "    if arr.ndim == 5 and arr.shape[0] == 1 and arr.shape[1] == 2:\n",
    "        return arr[0]\n",
    "    if arr.ndim == 5 and arr.shape[1] == 2:\n",
    "        if arr.shape[0] != 1:\n",
    "            raise ValueError(f\"Got batch N={arr.shape[0]} > 1; unexpected for test visualisation.\")\n",
    "        return arr[0]\n",
    "    if arr.ndim == 6 and arr.shape[1] == 1 and arr.shape[2] == 2:\n",
    "        return np.squeeze(arr, axis=0)[0]\n",
    "    raise ValueError(f\"Unsupported input shape: {arr.shape}\")\n",
    "\n",
    "def _get_slice(vol3d, axis=2, idx=\"center\"):\n",
    "    if idx == \"center\":\n",
    "        idx = vol3d.shape[axis] // 2\n",
    "    if axis == 0:\n",
    "        return vol3d[idx, :, :]\n",
    "    elif axis == 1:\n",
    "        return vol3d[:, idx, :]\n",
    "    elif axis == 2:\n",
    "        return vol3d[:, :, idx]\n",
    "    else:\n",
    "        raise ValueError(\"axis must be 0,1,2\")\n",
    "\n",
    "def _log1p_safe(a):\n",
    "    # Clip negatives to 0 for density-like fields, then log10(1+x)\n",
    "    return np.log10(1.0 + np.clip(a, 0, None))\n",
    "\n",
    "def _load_truth_inputs(test_file):\n",
    "    if not os.path.exists(test_file):\n",
    "        raise FileNotFoundError(f\"TEST_FILE not found: {test_file}\")\n",
    "    with h5py.File(test_file, \"r\") as f:\n",
    "        if \"input\" not in f or \"output_rho\" not in f:\n",
    "            raise KeyError(\"Required datasets 'input' and/or 'output_rho' missing in test file.\")\n",
    "        x = _ensure_input_shape(f[\"input\"][:])     # (2,D,H,W)\n",
    "        ngal = _safesqueeze(x[0])\n",
    "        vpec = _safesqueeze(x[1])\n",
    "        rho_true = _safesqueeze(f[\"output_rho\"][:])\n",
    "    return ngal, vpec, rho_true\n",
    "\n",
    "def _load_prediction(pred_file):\n",
    "    if not os.path.exists(pred_file):\n",
    "        raise FileNotFoundError(f\"Prediction file not found: {pred_file}\")\n",
    "    with h5py.File(pred_file, \"r\") as fp:\n",
    "        if \"prediction\" not in fp:\n",
    "            raise KeyError(f\"'prediction' dataset not found in {pred_file}\")\n",
    "        pred = _safesqueeze(fp[\"prediction\"][:])   # (D,H,W)\n",
    "    return pred\n",
    "\n",
    "def _candidate_loss_csvs(ckpt_dir):\n",
    "    # Common names to try, ordered by preference\n",
    "    names = [\n",
    "        \"train_log.csv\", \"history.csv\", \"loss.csv\", \"logs.csv\",\n",
    "        \"training_history.csv\", \"metrics.csv\"\n",
    "    ]\n",
    "    # Also any file matching loss*.csv or *history*.csv\n",
    "    existing = []\n",
    "    for n in names:\n",
    "        p = os.path.join(ckpt_dir, n)\n",
    "        if os.path.isfile(p):\n",
    "            existing.append(p)\n",
    "    for fname in sorted(os.listdir(ckpt_dir)):\n",
    "        if re.search(r\"(loss|history).*\\.csv$\", fname, re.IGNORECASE):\n",
    "            p = os.path.join(ckpt_dir, fname)\n",
    "            if p not in existing:\n",
    "                existing.append(p)\n",
    "    return existing\n",
    "\n",
    "def _load_loss_csv(loss_csv_path):\n",
    "    \"\"\"\n",
    "    Return dict: {\"epoch\": [...], \"train\": [...], \"val\": [...]}\n",
    "    Column names accepted (case-insensitive):\n",
    "      epoch, train_loss/train/TrainLoss, val_loss/val/ValLoss/valid/validation\n",
    "    \"\"\"\n",
    "    keys = {\"epoch\": None, \"train\": None, \"val\": None}\n",
    "    epochs, train, val = [], [], []\n",
    "    with open(loss_csv_path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        # map indices\n",
    "        col_map = {c.lower(): i for i, c in enumerate(header)}\n",
    "        # helper to find a column among aliases\n",
    "        def find_idx(*aliases):\n",
    "            for a in aliases:\n",
    "                if a in col_map:\n",
    "                    return col_map[a]\n",
    "            return None\n",
    "        ie = find_idx(\"epoch\", \"epochs\")\n",
    "        it = find_idx(\"train_loss\", \"train\", \"trainloss\", \"loss_train\", \"loss\")\n",
    "        iv = find_idx(\"val_loss\", \"val\", \"valloss\", \"valid\", \"validation\", \"loss_val\")\n",
    "        for row in reader:\n",
    "            try:\n",
    "                epochs.append(int(float(row[ie])) if ie is not None else len(epochs)+1)\n",
    "                train.append(float(row[it]) if it is not None else np.nan)\n",
    "                val.append(float(row[iv]) if iv is not None else np.nan)\n",
    "            except Exception:\n",
    "                # skip malformed lines\n",
    "                continue\n",
    "    return {\"epoch\": epochs, \"train\": train, \"val\": val}\n",
    "\n",
    "def _plot_one_model(tag, ngal, rho_true, pred, loss_dict, out_png):\n",
    "    # choose a consistent slice index across all models\n",
    "    ngal_s = _get_slice(ngal, axis=SLICE_AXIS, idx=SLICE_INDEX_MODE)\n",
    "    rho_t_s = _get_slice(rho_true, axis=SLICE_AXIS, idx=SLICE_INDEX_MODE)\n",
    "    rho_p_s = _get_slice(pred, axis=SLICE_AXIS, idx=SLICE_INDEX_MODE)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5.5))\n",
    "    # 1) Loss curve\n",
    "    ax0 = axes[0]\n",
    "    if loss_dict is not None and len(loss_dict.get(\"epoch\", [])) > 0:\n",
    "        ax0.plot(loss_dict[\"epoch\"], loss_dict[\"train\"], label=\"Train\", lw=1.8)\n",
    "        if not np.all(np.isnan(loss_dict[\"val\"])):\n",
    "            ax0.plot(loss_dict[\"epoch\"], loss_dict[\"val\"], label=\"Val\", lw=1.8)\n",
    "        ax0.set_xlabel(\"Epoch\")\n",
    "        ax0.set_ylabel(\"Loss\")\n",
    "        ax0.set_title(f\"[{tag}] Training Curve\")\n",
    "        ax0.grid(True, ls=\"--\", alpha=0.4)\n",
    "        ax0.legend()\n",
    "    else:\n",
    "        ax0.text(0.5, 0.5, \"No loss CSV found\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "        ax0.set_axis_off()\n",
    "\n",
    "    # 2) True slice\n",
    "    im1 = axes[1].imshow(_log1p_safe(rho_t_s), origin=\"lower\", cmap=\"inferno\")\n",
    "    axes[1].set_title(f\"[{tag}] True ρ (log10(1+ρ))\")\n",
    "    cb1 = plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cb1.ax.set_ylabel(\"log10(1+ρ)\")\n",
    "\n",
    "    # 3) Pred slice\n",
    "    im2 = axes[2].imshow(_log1p_safe(rho_p_s), origin=\"lower\", cmap=\"inferno\")\n",
    "    axes[2].set_title(f\"[{tag}] Predicted ρ̂ (log10(1+ρ̂))\")\n",
    "    cb2 = plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "    cb2.ax.set_ylabel(\"log10(1+ρ̂)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close(fig)\n",
    "    print(f\"[SAVE] {out_png}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load once: truth & inputs\n",
    "# -------------------------------\n",
    "ngal, vpec, rho_true = _load_truth_inputs(TEST_FILE)\n",
    "\n",
    "# -------------------------------\n",
    "# Iterate over three models\n",
    "# -------------------------------\n",
    "for tag in TAGS:\n",
    "    ckpt_dir = os.path.join(CKPT_BASE, tag)\n",
    "    pred_file = os.path.join(PRED_BASE, tag, f\"{INDEX}.hdf5\")\n",
    "\n",
    "    # Load prediction\n",
    "    try:\n",
    "        pred = _load_prediction(pred_file)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skip {tag}: cannot load prediction: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load loss curve if available\n",
    "    loss_dict = None\n",
    "    if os.path.isdir(ckpt_dir):\n",
    "        cands = _candidate_loss_csvs(ckpt_dir)\n",
    "        for cand in cands:\n",
    "            try:\n",
    "                ld = _load_loss_csv(cand)\n",
    "                if len(ld.get(\"epoch\", [])) > 0:\n",
    "                    loss_dict = ld\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "    else:\n",
    "        print(f\"[INFO] No ckpt dir for {tag}: {ckpt_dir}\")\n",
    "\n",
    "    # Output figure\n",
    "    out_png = os.path.join(OUT_DIR, f\"viz_{tag}_idx{INDEX}.png\")\n",
    "    _plot_one_model(tag, ngal, rho_true, pred, loss_dict, out_png)\n",
    "\n",
    "print(\"[DONE] All available models processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
